{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "from tqdm import tqdm_notebook\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from sklearn import tree\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "F_n = 10 # Fold Count\n",
    "\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from random import sample \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images loading\n",
    "def load_images_labels_RGB(img_folder, label_file):\n",
    "    with open(label_file) as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader) # skip header\n",
    "        labels = []\n",
    "        images = []\n",
    "        for row in reader:\n",
    "            img_filename = row[0]\n",
    "            img = Image.open(os.path.join(img_folder,img_filename))\n",
    "            if img is not None:\n",
    "                images.append(np.array(img))\n",
    "                labels.append(int(row[1]))\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Import data (normalized)\n",
    "Img_Data, Label_Data = load_images_labels_RGB('train/', 'train.csv')\n",
    "\n",
    "# Import data (normalized)\n",
    "Img_Data, Label_Data = load_images_labels_RGB('train/', 'train.csv')\n",
    "Size_Data = Img_Data.shape\n",
    "Fold_size = Size_Data[0]//F_n\n",
    "Mean_RGB = np.array([128.41563722, 115.24518493, 119.38645491])\n",
    "Std_RGB = np.array([38.55379149, 35.64913446, 39.07419321])\n",
    "Data_Norm = (Img_Data - Mean_RGB)/Std_RGB\n",
    "Data_NFlat = np.reshape(Data_Norm, (Size_Data[0], 32*32*3))\n",
    "\n",
    "train_df=pd.read_csv(\"../code/train.csv\")\n",
    "\n",
    "sns.countplot(train_df[\"has_cactus\"])\n",
    "train_df['has_cactus'].value_counts()\n",
    "\n",
    "train_dir = \"../code/train/\"\n",
    "test_dir = \"../code/test/\"\n",
    "\n",
    "image_id = train_df['id'].values\n",
    "class_label = train_df['has_cactus'].values\n",
    "\n",
    "data1 = pd.DataFrame(Data_NFlat)\n",
    "data2 = pd.DataFrame(class_label.tolist())\n",
    "data1[3072] = pd.Series(data2[0])\n",
    "\n",
    "mat_X_ = data1.values\n",
    "\n",
    "# np.savetxt('cactus_mat.txt',mat_X_,fmt='%.4f')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate covariance and perform eigenvalue decomposition\n",
    "def basis(data):\n",
    "    np.set_printoptions(formatter={'float':\"{:6.6g}\".format})\n",
    "\n",
    "    col = len (data.columns)-1\n",
    "    X = data.iloc[:,0:col].values\n",
    "    y = data.iloc[:,col].values\n",
    "    mat_X = np.matrix(X)    \n",
    "    mu = mat_X.sum(axis = 0)/(len(mat_X)) \n",
    "    mean = np.matrix(mu).T\n",
    "    cls = np.matrix(y).T\n",
    "\n",
    "    covariance_mat = (mat_X - mu).T.dot((mat_X - mu))/(mat_X.shape[0]-1)\n",
    "    print (covariance_mat.shape)\n",
    "    e_values, e_vectors = np.linalg.eig(covariance_mat)\n",
    "    e_pairs = [(np.abs(e_values[i]), e_vectors[:,i]) for i in range(len(e_values))]\n",
    "\n",
    "    # sort from high to low\n",
    "    e_pairs.sort()\n",
    "    e_pairs.reverse()\n",
    "    e_val_mat = np.asmatrix(e_values)\n",
    "    # sort the eigenvalue in the ascending order\n",
    "    e_val_mat.sort()\n",
    "\n",
    "    return mat_X, e_vectors, e_val_mat, e_pairs, cls, e_values\n",
    "\n",
    "mat_X, e_vectors, e_val_mat, e_pairs, cls, e_values= basis (data1)\n",
    "\n",
    "# derive a new set of basis and choose the major axes with variable error rate\n",
    "def PCA(eigen_val_mat,eigen_pairs,normalized_data,error_rate):\n",
    "    psum = 0.0\n",
    "    nf = np.size(eigen_val_mat,1)\n",
    "    for i in range (0, nf):\n",
    "        psum += eigen_val_mat[0, i]\n",
    "    p = 0\n",
    "    sum = 0.0\n",
    "    while sum/psum < error_rate and p < nf:\n",
    "        sum += eigen_val_mat[0,p]\n",
    "        p += 1   \n",
    "    pnf = nf - (p-1)  \n",
    "    print (pnf)\n",
    "    matrix_w = np.hstack((eigen_pairs[i][1].reshape(nf,1))for i in range (0,pnf))\n",
    "    return matrix_w\n",
    "\n",
    "# Represent the data using this new set of basis for a reduced dimension\n",
    "def dimension_reduction(W, data):\n",
    "    col = len (data.columns)-1\n",
    "    x = data.iloc[:,0:col].values\n",
    "    y = data.iloc[:,col].values\n",
    "    mat = np.matrix(x)\n",
    "    classs = np.matrix(y).T\n",
    "\n",
    "    reduced_data = (mat.dot(W))\n",
    "    pX = np.hstack((reduced_data.real,classs))\n",
    "#     print ('\\n Reduced Data: \\n', pX)\n",
    "    return pX\n",
    "\n",
    "# W = PCA (e_val_mat,e_pairs,mat_X,#)\n",
    "# pX = dimension_reduction (W, data1 )\n",
    "# np.savetxt('pFINAL_#.txt',pX,fmt='%.4f')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean-vectors for each class\n",
    "def mean_vectors(x, y):\n",
    "    class_ = np.unique(y)\n",
    "    mean_vectors = []\n",
    "    for cls in class_:\n",
    "        mean_vectors.append(np.mean(x[y==cls], axis=0))\n",
    "    return mean_vectors\n",
    "\n",
    "# calculate the within class scatter matrix \n",
    "def scatter_within(x, y):\n",
    "    class_ = np.unique(y)\n",
    "    n_col = x.shape[1]\n",
    "    mean = mean_vectors(x, y)\n",
    "    Sw = np.zeros((n_col, n_col))\n",
    "    for cls, mean_vec in zip(class_, mean):\n",
    "        Sc = np.zeros((n_col, n_col))                 \n",
    "        for row in x[y == cls]:\n",
    "            row, mean_vec = row.reshape(n_col, 1), mean_vec.reshape(n_col, 1)\n",
    "            Sc += (row-mean_vec).dot((row-mean_vec).T)\n",
    "        Sw += Sc   \n",
    "    return Sw\n",
    "\n",
    "# create W matrix\n",
    "def create_w (train_data):\n",
    "    col = len (train_data.columns)-1\n",
    "    x = train_data.iloc[:,0:col].values\n",
    "    y = train_data.iloc[:,col].values\n",
    "    mean_vec = mean_vectors(x, y)\n",
    "    Sw = scatter_within(x, y)\n",
    "    W = (np.linalg.inv(Sw)).dot(mean_vec[0]-mean_vec[1])\n",
    "    W_mat = np.matrix(W).T\n",
    "    return W_mat\n",
    "    \n",
    "# build LDA model with mean and Sw    \n",
    "def LDA(data, W_mat):\n",
    "    col = len (data.columns)-1\n",
    "    x = data.iloc[:,0:col].values\n",
    "    y = data.iloc[:,col].values\n",
    "    mat = np.matrix(x)\n",
    "    classs = np.matrix(y).T\n",
    "\n",
    "    reduced_data = mat.dot(W_mat)\n",
    "    fX = np.hstack((reduced_data.real,classs))\n",
    "#     print ('\\n Reduced Data: \\n', fX)\n",
    "    return fX\n",
    "\n",
    "# W_mat = create_w (data1)\n",
    "# fX = LDA (data1,W_mat )\n",
    "# np.savetxt('fFINAL.txt',fX,fmt='%.8f')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised\n",
    "### MPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mle (data,a,b):\n",
    "    df = pd.DataFrame(data,index = None, columns = list(range (len(data[0]))) )\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    col = len (df.columns)-1\n",
    "    X = df.iloc[:,0:col].values\n",
    "    Y = df.iloc[:,col].values\n",
    "        \n",
    "    class0 = df.loc[(df[col] == 0)]\n",
    "    class1 = df.loc[(df[col] == 1)]\n",
    "\n",
    "    col0 = len (class0.columns)-1\n",
    "    X0 = class0.iloc[:,0:col0].values\n",
    "    mat0 = np.matrix(X0)    \n",
    "\n",
    "    col1 = len (class1.columns)-1\n",
    "    X1 = class1.iloc[:,0:col1].values\n",
    "    mat1 = np.matrix(X1)  \n",
    "\n",
    "    class_type = np.unique(Y)\n",
    "    n_classes = class_type.shape[0]\n",
    "    mean_vectors = []\n",
    "    for cls in class_type:\n",
    "        mean_vectors.append(np.mean(X[Y==cls], axis=0))\n",
    "    \n",
    "    mu0 = mean_vectors[0]\n",
    "    mu1 = mean_vectors[1]\n",
    "    cov0 = ((mat0 - mu0).T.dot((mat0 - mu0)))/mat0.shape[0]\n",
    "    cov1 = ((mat1 - mu1).T.dot((mat1 - mu1)))/mat1.shape[0]\n",
    "\n",
    "    # find covariance matrices for CASE I\n",
    "    I = np.identity(col)\n",
    "    sigma3 = cov0[a,b]*I\n",
    "\n",
    "    cov_vectors = [cov0,cov1,sigma3] \n",
    "    dim_data = df.shape[0]\n",
    "    dim_class0 = class0.shape[0]\n",
    "    dim_class1 = class1.shape[0]\n",
    "\n",
    "    return mean_vectors, cov_vectors, dim_data, dim_class0,dim_class1\n",
    "\n",
    "\n",
    "#implement mpp\n",
    "def mpp(data_te,mean1,mean2,detClass1,detClass2,invClass1,invClass2,probClass1):\n",
    "    probClass2 = 1-probClass1\n",
    "    df = pd.DataFrame(data_te,index = None, columns = list(range (len(data_te[0]))) )\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    col = len (df.columns)\n",
    "    X = df.iloc[:,0:col].values\n",
    "\n",
    "    # create a matrix from test data for calculation\n",
    "    testSet = np.matrix(X)\n",
    "\n",
    "    n = testSet.shape[1]\n",
    "    \n",
    "    predictedMatrix = np.zeros((testSet.shape[0], 2))\n",
    "    \n",
    "    bestAccuracy = 0\n",
    "    correctGuesses = 0\n",
    "    line1 = np.zeros((1, n-1))\n",
    "    for i in range(testSet.shape[0]):\n",
    "        for j in range (n-1):\n",
    "            line1[:,j] = testSet[i,j]\n",
    "\n",
    "        line2 = line1.T\n",
    "        mahalanobis = line2 - mean1\n",
    "        mahalanobis1 = mahalanobis.T\n",
    "        mahalanobis2 = mahalanobis1 * invClass1\n",
    "        mahalanobis3 = mahalanobis2 * (line2 - mean1)\n",
    "        varMahalanobis = float(-0.5 * mahalanobis3[0][0])\n",
    "        probIsClass1 = float((1.0 / math.sqrt(2 * math.pi * detClass1)) * math.exp(varMahalanobis) * (probClass1))\n",
    "\n",
    "        mahalanobiss = line2 - mean2\n",
    "        mahalanobiss1 = mahalanobiss.T\n",
    "        mahalanobiss2 = mahalanobiss1 * invClass2\n",
    "        mahalanobiss3 = mahalanobiss2 * (line2 - mean2)\n",
    "        varMahalanobiss = float(-0.5 * mahalanobiss3[0][0])\n",
    "\n",
    "        probIsClass2 = float((1.0 / math.sqrt(2 * math.pi * detClass2)) * math.exp(varMahalanobiss) * (probClass2))\n",
    "\n",
    "        predictedClass = 0\n",
    "        if probIsClass2 > probIsClass1:\n",
    "            predictedClass = 1\n",
    "\n",
    "        if predictedClass == testSet[i,n-1]: \n",
    "            correctGuesses+=1\n",
    "\n",
    "        error = float(min(probIsClass2, probIsClass1))\n",
    "\n",
    "        predictedMatrix[i][1] = error\n",
    "        predictedMatrix[i][0] = predictedClass\n",
    "            \n",
    "#         print (correctGuesses) \n",
    "#     print (len(testSet))\n",
    "    acc = float(correctGuesses) / float(len(testSet))\n",
    "    if acc > bestAccuracy:\n",
    "        bestAccuracy = acc\n",
    "\n",
    "    return testSet,predictedMatrix, bestAccuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate TP,TN,FP,FN values\n",
    "def performance_measure(testSet, predictedMatrix):\n",
    "    n = testSet.shape[1]\n",
    "    true_class = []\n",
    "    predicted_class = []\n",
    "\n",
    "    for i in range(testSet.shape[0]):\n",
    "        true = testSet[i,n-1]\n",
    "        true_class.append(true)\n",
    "        \n",
    "    for i in range(predictedMatrix.shape[0]):\n",
    "        predicted = predictedMatrix[i,0]\n",
    "        predicted_class.append(predicted)\n",
    "        \n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    posneg = []\n",
    "    for i in range(len(predicted_class)): \n",
    "        if true_class[i]==predicted_class[i]==1:\n",
    "            TP += 1\n",
    "        if predicted_class[i]==1 and true_class[i]!=predicted_class[i]:\n",
    "            FP += 1\n",
    "        if true_class[i]==predicted_class[i]==0:\n",
    "            TN += 1\n",
    "        if predicted_class[i]==0 and true_class[i]!=predicted_class[i]:\n",
    "            FN += 1\n",
    "\n",
    "    posneg.append(TP)\n",
    "    posneg.append(FP)\n",
    "    posneg.append(TN)\n",
    "    posneg.append(FN)\n",
    "    \n",
    "    \n",
    "    return posneg\n",
    "\n",
    "#calculate sensitivity,specificity, TPR, FPR\n",
    "def accuracy(performance_list):\n",
    "    sensitivity = performance_list[0]/(performance_list[0]+performance_list[3])\n",
    "    specificity = performance_list[2]/(performance_list[2]+performance_list[1])\n",
    "    TPR = sensitivity\n",
    "    FPR = 1 - specificity\n",
    "    roc = [sensitivity,specificity, TPR, FPR]\n",
    "    return roc\n",
    "\n",
    "#calculate accuracy (the probability of a correct decision)\n",
    "def getAccuracy(performance_list):\n",
    "    acc = (performance_list[0]+performance_list[2])/sum(performance_list)\n",
    "    \n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-fold validation on the training set\n",
    "#split the training set into train and test \n",
    "def kfold_split(n_split, x_len):\n",
    "    index = []\n",
    "    test_size = int(x_len/n_split)\n",
    "    train_size = x_len - test_size\n",
    "    for i in range(n_split):\n",
    "        j = i*test_size\n",
    "        index.append([list(set(list(range(0,x_len))).difference(set(list(range(j,j+test_size)))) ),\n",
    "                     list(range(j,j+test_size))])\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement MPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openfile (filename):\n",
    "    fileData1 = open(filename, \"r\")#,encoding='utf-8-sig')\n",
    "    lines = fileData1.readlines()\n",
    "\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        x = line.split(',')\n",
    "        data.append(x)\n",
    "    return data\n",
    "\n",
    "def det (mat,n):\n",
    "    (sign, logdet) = np.linalg.slogdet(mat)\n",
    "    s = sign * np.exp(logdet/n)\n",
    "    return s\n",
    "\n",
    "#get mean accuracy of all K-folds for diffrent k in KNN\n",
    "def cross_validation_score(trainingSet, n_split,n):\n",
    "    train = pd.DataFrame(trainingSet)\n",
    "    col = len (train.columns)\n",
    "    X = train.iloc[:,0:col].values\n",
    "    index = kfold_split(n_split, len(X))\n",
    "    acc1 = []\n",
    "    acc2 = []\n",
    "    acc3 = []\n",
    "    \n",
    "    t1 = []\n",
    "    f1 = []\n",
    "    t2 = []\n",
    "    f2 = []\n",
    "    t3 = []\n",
    "    f3 = []\n",
    "\n",
    "    count = 0\n",
    "    for train_index, test_index in index:\n",
    "        count += 1\n",
    "        print ('\\nfold: ',count)\n",
    "        \n",
    "        x = X[train_index]\n",
    "        y = X[test_index]\n",
    "        \n",
    "        # uncomment this line for output\n",
    "#         print (x.tolist())\n",
    "        mean_vectors, cov_vectors,dim_data, dim_class0,dim_class1  = gaussian_mle (x.tolist(),0,0)\n",
    "        mean1 = np.matrix(mean_vectors[0]).T\n",
    "        mean2 = np.matrix(mean_vectors[1]).T\n",
    "        cov0 = cov_vectors[0]\n",
    "        cov1 = cov_vectors[1]\n",
    "        cov2 = cov_vectors [2]\n",
    "\n",
    "        # calculate determinant and inverse of new covariance Matrix (case 1: covariance matrices are equal to (sigma^2)I)\n",
    "        detClass1 = det (cov2,n) #np.linalg.det(cov2)\n",
    "        detClass2 = detClass1\n",
    "        invClass1 = np.linalg.inv(cov2)\n",
    "        invClass2 = invClass1\n",
    "\n",
    "        # calculate determinant and inverse of new covariance matrices (case 2: covariance matrices is equal)\n",
    "        detClass12 = det (cov1,n) #np.linalg.det(cov1)\n",
    "        detClass22 = detClass12\n",
    "        invClass12 = np.linalg.inv(cov1)\n",
    "        invClass22 = invClass12\n",
    "        \n",
    "        # calculate determinant and inverse of new covariance matrices (case 3: covariance matrices are different)\n",
    "        detClass13 = det (cov0,n) #np.linalg.det(cov0)\n",
    "        invClass13 = np.linalg.inv(cov0)\n",
    "        invClass23 = np.linalg.inv(cov1)\n",
    "        detClass23 = det (cov1,n) #np.linalg.det(cov1)\n",
    "#         print (detClass13)\n",
    "#         print (detClass23)\n",
    "\n",
    "        testSet1,predictedMatrix1,bestAccuracy1 = mpp(y.tolist(),mean1,mean2,detClass1,detClass2,invClass1,invClass2,dim_class0/dim_data)\n",
    "        case1 = performance_measure(testSet1, predictedMatrix1)\n",
    "        per_list1 = accuracy(case1)\n",
    "        \n",
    "        classificationAccuracy1 = getAccuracy(case1)\n",
    "        tpr1 = accuracy(per_list1)[2]\n",
    "        fpr1 = accuracy(per_list1)[3]\n",
    "        t1.append(tpr1)\n",
    "        f1.append(fpr1)\n",
    "        print (\"\\nCASE I\")\n",
    "        print (classificationAccuracy1)\n",
    "        acc1.append(classificationAccuracy1)\n",
    "#         class_acc1+=bestAccuracy1\n",
    "        \n",
    "        testSet2,predictedMatrix2,bestAccuracy2 = mpp(y.tolist(), mean1,mean2,detClass12,detClass22,invClass12,invClass22,dim_class0/dim_data)\n",
    "        case2 = performance_measure(testSet2, predictedMatrix2)\n",
    "        per_list2 = accuracy(case2)\n",
    "        classificationAccuracy2 = getAccuracy(case2)\n",
    "        roc2 = accuracy(per_list2)\n",
    "        tpr2 = accuracy(per_list2)[2]\n",
    "        fpr2 = accuracy(per_list2)[3]\n",
    "        t2.append(tpr2)\n",
    "        f2.append(fpr2)\n",
    "        print (\"\\nCASE II\")\n",
    "        print (classificationAccuracy2)\n",
    "#         print (bestAccuracy2)\n",
    "        \n",
    "        acc2.append(classificationAccuracy2)\n",
    "#         class_acc2+=bestAccuracy2\n",
    "\n",
    "        testSet3,predictedMatrix3,bestAccuracy3 = mpp(y.tolist(),mean1,mean2,detClass13,detClass23,invClass13,invClass23,dim_class0/dim_data)\n",
    "        case3 = performance_measure(testSet3, predictedMatrix3)\n",
    "        per_list3 = accuracy(case3)\n",
    "        tpr3 = accuracy(per_list3)[2]\n",
    "        fpr3 = accuracy(per_list3)[3]\n",
    "\n",
    "        classificationAccuracy3 = getAccuracy(case3)\n",
    "        roc3 = accuracy(per_list3)\n",
    "        t3.append(tpr3)\n",
    "        f3.append(fpr3)\n",
    "        print (\"\\nCASE III\")\n",
    "        print (classificationAccuracy3)\n",
    "        acc3.append(classificationAccuracy3)\n",
    "\n",
    "    avg1 = (sum(acc1)/len(acc1))*100\n",
    "    print('\\n CASE I accuracy: ',avg1)\n",
    "    print ('\\n CASE I TPR: ', t1)\n",
    "    print ('\\n CASE I FPR: ', f1)\n",
    "\n",
    "    avg2 = (sum(acc2)/len(acc2))*100\n",
    "    print('\\n CASE II accuracy: ',avg2)\n",
    "    print ('\\n CASE II TPR: ', t2)\n",
    "    print ('\\n CASE II FPR: ', f2)\n",
    "\n",
    "    avg3 = (sum(acc3)/len(acc3))*100\n",
    "    print('\\n CASE III accuracy: ',avg3)\n",
    "    print ('\\n CASE III TPR: ', t3)\n",
    "    print ('\\n CASE III FPR: ', f3)\n",
    "\n",
    "    return avg1,avg2,avg3\n",
    "\n",
    "# data2 = openfile ('pFINAL_.01.txt')\n",
    "# cross_validation_score(data2, 5,100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised\n",
    "### k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openfile (filename):\n",
    "    fileData1 = open(filename, \"r\")#,encoding='utf-8-sig')\n",
    "    lines = fileData1.readlines()\n",
    "\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        x = line.split()\n",
    "        data.append(x)\n",
    "    n = len(data[0])-1\n",
    "    df1 = pd.DataFrame(data)\n",
    "    df = df1[df1.columns[:-1]]\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    d = df1[[n]].copy()\n",
    "    cls = d[n]\n",
    "    col=len(df)\n",
    "    A = df.iloc[:,0:col].values\n",
    "\n",
    "    return A,cls\n",
    "\n",
    "def euclidean_dist(point1, point2):\n",
    "    d = np.sum(np.square(point2-point1))\n",
    "    return (np.sqrt(d))\n",
    "\n",
    "\"\"\"Takes the X matrix and the centroids. Then measures the shortest \n",
    "distances between them and returns a vector of cluster indices per sample in X\"\"\"\n",
    "def closest_centroids(X, centroid):\n",
    "    id_ = np.zeros((X.shape[0],1))\n",
    "    \n",
    "    #Loop through each sample in X\n",
    "    for x in range(id_.shape[0]):\n",
    "        point = X[x]\n",
    "        #measure distance of this point from each centroid,\n",
    "        #Keep track of shortest distance and index of shortest distance\n",
    "        min_dist, idx = 255, 0\n",
    "        for i in range(centroid.shape[0]):\n",
    "            centroid_ = centroid[i]\n",
    "            dist = euclidean_dist(centroid_,point)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                idx = i\n",
    "        #modify the index vector with new id\n",
    "        id_[x] = idx\n",
    "        \n",
    "    return id_\n",
    "\n",
    "\n",
    "\"\"\"Takes X matrix and index vector to compute new centroids\"\"\"\n",
    "def new_centroids(X, id_):\n",
    "    new = []\n",
    "    for x in range(len(np.unique(id_))):\n",
    "        new.append(np.array([X[i] for i in range(X.shape[0]) if id_[i] == x]))\n",
    "    return np.array([np.mean(y,axis=0) for y in new])\n",
    "\n",
    "\n",
    "\"\"\"Implement k-means all at once \"\"\"\n",
    "def kmeans(X, initial_centroids, k,n_iter):\n",
    "    current_centroids = initial_centroids\n",
    "    id_ = closest_centroids(X,current_centroids)\n",
    "    current_centroids = new_centroids(X,id_)\n",
    "    curr_cent = np.array (current_centroids)\n",
    "    prev_cent = np.array(initial_centroids)\n",
    "#     print (curr_cent)\n",
    "    ite = 0\n",
    "    while not np.array_equal (prev_cent,curr_cent) and ite < n_iter:\n",
    "        ite += 1\n",
    "        print (ite)\n",
    "        prev_centroids = []\n",
    "        prev_centroids.append(current_centroids)\n",
    "        prev_cent = np.array(prev_centroids)\n",
    "        id_ = closest_centroids(X,current_centroids)\n",
    "        current_centroids = new_centroids(X,id_)\n",
    "        curr_cent = np.array (current_centroids)\n",
    "        \n",
    "    return id_, prev_centroids\n",
    "\n",
    "\"\"\"Initialize k random centroids\"\"\"\n",
    "def k_random_centroids(X, k):\n",
    "    random_indices = sample(range(0,X.shape[0]),k)\n",
    "    return np.array([X[i] for i in random_indices])\n",
    "\n",
    "\n",
    "def clustering(A,k,n_iter):\n",
    "    id_, prev_centroids = kmeans(A,k_random_centroids(A,k), k,n_iter)\n",
    "    id_ = closest_centroids(A, prev_centroids[-1])\n",
    "    return id_\n",
    "\n",
    "#calculate TP,TN,FP,FN values\n",
    "def performance_measure(true_label, clustered_output):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    posneg = []\n",
    "    for i in range(len(clustered_output)): \n",
    "        if true_label[i]==clustered_output[i]==1:\n",
    "            TP += 1\n",
    "        if clustered_output[i]==1 and true_label[i]!=clustered_output[i]:\n",
    "            FP += 1\n",
    "        if true_label[i]==clustered_output[i]==0:\n",
    "            TN += 1\n",
    "        if clustered_output[i]==0 and true_label[i]!=clustered_output[i]:\n",
    "            FN += 1\n",
    "\n",
    "    posneg.append(TP)\n",
    "    posneg.append(FP)\n",
    "    posneg.append(TN)\n",
    "    posneg.append(FN)\n",
    "    \n",
    "    confusion_mat = np.array([[TN,FP],[FN,TP]])\n",
    "    return posneg, confusion_mat\n",
    "\n",
    "\n",
    "#calculate sensitivity,specificity, TPR, FPR\n",
    "def perf_list(true_label, clustered_output):\n",
    "    performance_list,confusion_mat = performance_measure(true_label, clustered_output)\n",
    "    sensitivity = performance_list[0]/(performance_list[0]+performance_list[3])\n",
    "    specificity = performance_list[2]/(performance_list[2]+performance_list[1])\n",
    "    TPR = sensitivity\n",
    "    FPR = 1 - specificity\n",
    "    roc = [sensitivity,specificity, TPR, FPR]\n",
    "    return roc\n",
    "\n",
    "\n",
    "#calculate accuracy (the probability of a correct decision)\n",
    "def accuracy_score(true_label, clustered_output):\n",
    "    performance_list,confusion_mat = performance_measure(true_label, clustered_output)\n",
    "    acc = (performance_list[0]+performance_list[2])/sum(performance_list)\n",
    "    return acc, confusion_mat\n",
    "\n",
    "def ground_truth(label):\n",
    "    true_label = (label.apply(pd.to_numeric)).tolist()\n",
    "    return true_label\n",
    "\n",
    "def accuracy (filename):\n",
    "    A,label = openfile(filename)\n",
    "    predictions = clustering (A,2,10)\n",
    "    km_out = [x[0] for x in predictions.tolist()]\n",
    "    km_output_changed = (pd.DataFrame(km_out)).replace([1, 0], [0, 1])\n",
    "    km_output_changed = [x[0] for x in km_output_changed.values.tolist()]\n",
    "    true_label = ground_truth(label)\n",
    "    roc_org = perf_list(true_label, km_out)\n",
    "    roc_chng = perf_list(true_label, km_output_changed)\n",
    "    ac_result,confusion_mat_org = accuracy_score(true_label, km_out)\n",
    "    ac_manipulated,confusion_mat_manipulated = accuracy_score(true_label, km_output_changed)\n",
    "    print ('ROC_org: ', roc_org)\n",
    "    print ('ROC_flipped: ', roc_chng)\n",
    "    print ('accuracy_result: ',ac_result*100)\n",
    "    print ('accuracy_manipulated_result: ',ac_manipulated*100)\n",
    "    return ac_result*100,ac_manipulated*100\n",
    "\n",
    "\n",
    "# accuracy('fFINAL.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### winner-take-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(point1, point2):\n",
    "    d = np.sum(np.square(point2-point1))\n",
    "    return (np.sqrt(d))\n",
    "\n",
    "def closest_centroids_winner(X, centroid,k):\n",
    "    id_ = np.zeros((X.shape[0],1))\n",
    "    id_ = np.zeros((X.shape[0],1))\n",
    "    #Loop through each data point in X\n",
    "\n",
    "    for x in range(id_.shape[0]):\n",
    "        point = X[x]\n",
    "        #Compare this point to each centroid,\n",
    "        #Keep track of shortest distance and index of shortest distance\n",
    "        min_dist, idx = 255, 0\n",
    "        for i in range(k):\n",
    "            if centroid is None:\n",
    "                random_indices = sample(range(0,X.shape[0]),k)\n",
    "                centroid = np.array([X[i] for i in random_indices])\n",
    "                centroid_ = centroid[i] \n",
    "            elif centroid[i].size==0:\n",
    "                centroid[i] = np.zeros((1,3))\n",
    "                centroid_ = centroid[i] \n",
    "            else:\n",
    "                centroid_ = centroid[i]\n",
    "            dist = euclidean_dist(centroid_,point)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                idx = i\n",
    "#         print (centroid_)\n",
    "        #With the best index found, modify the result idx vector\n",
    "        id_[x] = idx\n",
    "                \n",
    "    return id_\n",
    "\n",
    "\n",
    "# calculate new centroids\n",
    "def new_centroids_winner(X, id_,prev_centroids,k):\n",
    "    prev_cen = prev_centroids[0].tolist()\n",
    "    n = k\n",
    "    new = []\n",
    "    for x in range(n):\n",
    "        a = [X[i] for i in range(X.shape[0]) if id_[i] == x]\n",
    "        new.append(a)\n",
    "    \n",
    "    \n",
    "    new_cen = []\n",
    "    for x in range (n):\n",
    "        new_ = []\n",
    "        for y in new[x]:\n",
    "            part1 = np.array(prev_cen[x])\n",
    "            part2_ = y-part1\n",
    "            part2 = 0.001*part2_\n",
    "            new__ = part1 + (part2)\n",
    "            new_.append(np.array(new__))\n",
    "        mean =  np.array([sum(y)/len(y) for y in new_])\n",
    "#         print (mean)\n",
    "        new_cen.append(mean)\n",
    "#     print (new_cen)\n",
    "    return np.array(new_cen)\n",
    "\n",
    "# Implement winner-take-all all at once\n",
    "def winner_take_all(X, initial_centroids, k, n_iter):\n",
    "    prev_centroids = []\n",
    "    current_centroids = initial_centroids\n",
    "    prev_centroids.append(current_centroids)\n",
    "    prev_cent = np.array(prev_centroids)\n",
    "    id_ = closest_centroids_winner(X,current_centroids,k)\n",
    "    current_centroids = new_centroids_winner(X,id_,prev_centroids,k)\n",
    "    curr_cent = np.array(current_centroids)\n",
    "    \n",
    "    ite = 0\n",
    "    while not np.array_equal (prev_cent,curr_cent) and ite < n_iter:\n",
    "        ite += 1\n",
    "        print (ite)\n",
    "        prev_centroids = []\n",
    "        current_centroids = initial_centroids\n",
    "        prev_centroids.append(current_centroids)\n",
    "        prev_cent = np.array(prev_centroids)\n",
    "        id_ = closest_centroids_winner(X,current_centroids,k)\n",
    "        current_centroids = new_centroids_winner(X,id_,prev_centroids,k)\n",
    "#         print (prev_centroids)\n",
    "        curr_cent = current_centroids\n",
    "\n",
    "#     print (prev_centroids)\n",
    "    return id_, prev_centroids\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\"Initialize k random centroids\"\"\"\n",
    "def k_random_centroids(X, k):\n",
    "    random_indices = sample(range(0,X.shape[0]),k)\n",
    "    return np.array([X[i] for i in random_indices])\n",
    "\n",
    "\n",
    "def clustering_winner(A,k,n_iter):\n",
    "    id_, prev_centroids = winner_take_all(A,k_random_centroids(A,k), k,n_iter)\n",
    "    id_ = closest_centroids(A, prev_centroids[-1])\n",
    "    return id_\n",
    "\n",
    "\n",
    "def accuracy_winner (filename):\n",
    "    A,label = openfile(filename)\n",
    "    predictions = clustering_winner (A,2,10)\n",
    "    km_out = [x[0] for x in predictions.tolist()]\n",
    "    km_output_changed = (pd.DataFrame(km_out)).replace([1, 0], [0, 1])\n",
    "    km_output_changed = [x[0] for x in km_output_changed.values.tolist()]\n",
    "    true_label = ground_truth(label)\n",
    "    roc_org = perf_list(true_label, km_out)\n",
    "    roc_chng = perf_list(true_label, km_output_changed)\n",
    "    ac_result = accuracy_score(true_label, km_out)\n",
    "    ac_manipulated = accuracy_score(true_label, km_output_changed)\n",
    "    print ('ROC_org: ', roc_org)\n",
    "    print ('ROC_flipped: ', roc_chng)\n",
    "    print ('accuracy_result: ',ac_result*100)\n",
    "    print ('accuracy_manipulated_result: ',ac_manipulated*100)\n",
    "    return ac_result*100,ac_manipulated*100\n",
    "\n",
    "# accuracy_winner ('cactus_mat.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist_(point1,point2):\n",
    "    difference = point1-point2\n",
    "    if (difference.ndim ==1):\n",
    "        return np.abs(difference)\n",
    "    return np.linalg.norm(difference,axis = 1)\n",
    "\n",
    "# function for SOM\n",
    "def SOM (A, variance, min_eps, max_eps, k):\n",
    "    min_ = 0\n",
    "    max_ = 255\n",
    "    n_iter = 10\n",
    "    sqrt_k = int (np.ceil(np.sqrt(k)))\n",
    "    prev_centroids  = np.array([0 for j in range(len(A))])\n",
    "    new_A = np.array([1 for j in range(len(A))])\n",
    "    \n",
    "    # create grid of centroids\n",
    "    new_centroids = np.array([[np.random.uniform(min_, max_) for i in range (len(A[0]))]for j in range(k)])\n",
    "    indices = np.array([ind for ind in range(k)])\n",
    "\n",
    "    new_centroids = np.array(new_centroids)\n",
    "    ite = 0\n",
    "    while not np.array_equal(prev_centroids,new_A) and ite < n_iter:\n",
    "        prev_centroids = np.copy(new_A)\n",
    "        \n",
    "        ite += 1\n",
    "        print (ite)\n",
    "        epsilon = max_eps*(min_eps/max_eps)**(ite/n_iter)\n",
    "        for i in range(0,len(A)):\n",
    "            distance = euclidean_dist_(new_centroids,A[i])\n",
    "            n = np.column_stack((indices,distance))\n",
    "            n = n[n[:,1].argsort()]\n",
    "            cluster_indices = int(n[0,0])            \n",
    "            n = n[n[:,1] <= variance]\n",
    "            for j in range(len(n)):\n",
    "                phi = np.exp(-np.linalg.norm(new_centroids[int(n[j,0])]-new_centroids[cluster_indices])**2/(2*variance**2))\n",
    "                new_centroids[int(n[j,0])] = new_centroids[int(n[j,0])]+epsilon*phi*(np.subtract(A[i],new_centroids[cluster_indices]))\n",
    "                \n",
    "            new_A [i] = cluster_indices\n",
    "\n",
    "    new_centroids = np.array([[int(np.round(new_centroids[j,i]))for i in range(len(A[0]))]for j in range(k)])\n",
    "    return np.array(new_A),new_centroids\n",
    "    \n",
    "\n",
    "def clustering_SOM(A,k):\n",
    "    new_A,new_centroids = SOM (A, 10.0, 0.0001,0.1,k)\n",
    "    print (new_A)\n",
    "    return new_A\n",
    "\n",
    "def accuracy_som (filename):\n",
    "    A,label = openfile(filename)\n",
    "    km_out = clustering_SOM (A,2)\n",
    "#     print(predictions)\n",
    "#     km_out = [x[0] for x in predictions.tolist()]\n",
    "    km_output_changed = (pd.DataFrame(km_out)).replace([1, 0], [0, 1])\n",
    "    km_output_changed = [x[0] for x in km_output_changed.values.tolist()]\n",
    "    true_label = ground_truth(label)\n",
    "    roc_org = perf_list(true_label, km_out)\n",
    "    roc_chng = perf_list(true_label, km_output_changed)\n",
    "    ac_result = accuracy_score(true_label, km_out)\n",
    "    ac_manipulated = accuracy_score(true_label, km_output_changed)\n",
    "    print ('ROC_org: ', roc_org)\n",
    "    print ('ROC_flipped: ', roc_chng)\n",
    "    print ('accuracy_result: ',ac_result*100)\n",
    "    print ('accuracy_manipulated_result: ',ac_manipulated*100)\n",
    "    return ac_result*100,ac_manipulated*100\n",
    "\n",
    "# accuracy_som('fFINAL.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BKS Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BKS(confusionMat1, confusionMat2,predicted1,predicted2):\n",
    "    clf1 = openfile (confusionMat1)\n",
    "    clf2 = openfile (confusionMat2)\n",
    "    clf1_out = openfile_mpp(predicted1)\n",
    "    clf2_out = openfile(predicted2)\n",
    "    clf1_predicted = [int(x[0]) for x in clf1_out]\n",
    "    clf2_predicted = [int(x[0]) for x in clf2_out]\n",
    "    \n",
    "    class0 = []\n",
    "    for i in range (2):\n",
    "        for x in clf2[0]:\n",
    "            class0.append(int(clf1[0][i])+int(x)) \n",
    "\n",
    "#     print (class0)\n",
    "\n",
    "    class1 = []\n",
    "    for i in range (2):\n",
    "        for x in clf2[1]:\n",
    "            class1.append(int(clf1[1][i])+int(x)) \n",
    "\n",
    "#     print (class1)\n",
    "    \n",
    "    fused_label = np.zeros((1,4))\n",
    "    for i in range (4):\n",
    "        if class0[i] > class1[i]:\n",
    "            fused_label[0,i] = 0\n",
    "        else:\n",
    "            fused_label[0,i] = 1\n",
    "\n",
    "#     print (fused_label)\n",
    "    new_label = np.zeros((len(clf1_predicted),1))\n",
    "\n",
    "    for i in range (len(clf1_predicted)):\n",
    "        if clf1_predicted[i] == 0 and clf2_predicted[i] == 0:\n",
    "            new_label[i,0] = fused_label[0,0]\n",
    "        elif clf1_predicted[i] == 0 and clf2_predicted[i] == 1:\n",
    "            new_label[i,0] = fused_label[0,1]\n",
    "        elif clf1_predicted[i] == 1 and clf2_predicted[i] == 0:\n",
    "            new_label[i,0] = fused_label[0,2]\n",
    "        elif clf1_predicted[i] == 1 and clf2_predicted[i] == 1:\n",
    "            new_label[i,0] = fused_label[0,3]\n",
    "            \n",
    "    return new_label\n",
    "\n",
    "# new_label1 = BKS('f_knn.csv', 'f_dt.csv','f_knn_pred.csv','f_dt_pred.csv')\n",
    "# accuracy_score (testSet, new_label1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
